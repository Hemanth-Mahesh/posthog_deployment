# PostHog Self-Hosted Deployment Guide for DriveX

**Domain:** `posthog.drivex.dev`  
**Server:** AWS EC2 (Ubuntu 24.04)  
**Date:** November 24, 2025

---

## Overview

This document details the complete process of deploying PostHog on an EC2 instance with SSL/TLS enabled via Let's Encrypt, accessible at `https://posthog.drivex.dev`.

---

## Initial Problem

PostHog was running locally on the EC2 instance but was inaccessible from the external domain `posthog.drivex.dev`. The browser showed:

- **Error:** `SSL_ERROR_INTERNAL_ERROR_ALERT`
- **Cause:** PostHog was configured for `localhost` instead of the actual domain

---

## Issues Fixed

### 1. Domain Configuration in `.env`

**Problem:** The `.env` file was configured for localhost.

**Original Configuration:**
```env
DOMAIN=localhost
CADDY_HOST="localhost, http://, https://"
```

**Fixed Configuration:**
```env
POSTHOG_SECRET=<your-secret>
ENCRYPTION_SALT_KEYS=<your-salt>
DOMAIN=posthog.drivex.dev
EXTERNAL_URL=https://posthog.drivex.dev
POSTHOG_APP_TAG=latest
REGISTRY_URL=posthog/posthog
TLS_BLOCK=
```

**Location:** `~/posthog/.env`

---

### 2. Docker Port Binding Conflict (Port 8333)

**Problem:** SeaweedFS container failed to start due to port 8333 being in use.

**Error:**
```
Error response from daemon: failed to bind host port 127.0.0.1:8333/tcp: address already in use
```

**Solution:** Complete Docker cleanup and restart.

```bash
# Stop all containers
sudo docker compose -f docker-compose.hobby.yml down -v --remove-orphans

# Clean up all Docker resources
sudo docker system prune -af --volumes

# Restart Docker daemon
sudo systemctl restart docker

# Wait and restart
sleep 10
sudo docker compose -f docker-compose.hobby.yml up -d
```

---

### 3. Corrupted Git Clone - Files as Directories

**Problem:** Git checkout was corrupted - `config.xml` and other files were directories instead of files.

**Symptom:**
```
drwxr-xr-x 2 root root 4096 Nov 24 07:25 config.xml   # Should be a file, not directory
```

**Solution:** Fresh clone of the repository.

```bash
cd ~
sudo rm -rf ~/posthog
git clone https://github.com/PostHog/posthog.git
cd posthog
```

---

### 4. Incorrect Volume Mount Paths in docker-compose

**Problem:** The `docker-compose.hobby.yml` had incorrect paths pointing to `./posthog/docker/` instead of `./docker/`.

**Error:**
```
error mounting "/home/ubuntu/posthog/posthog/docker/clickhouse/config.xml"
```

**Solution:** Fix paths using sed.

```bash
cd ~/posthog
sed -i 's|./posthog/docker/|./docker/|g' docker-compose.hobby.yml
sed -i 's|./posthog/posthog/|./posthog/|g' docker-compose.hobby.yml
```

**Verification:**
```bash
grep -n "clickhouse" ~/posthog/docker-compose.hobby.yml | head -10
# Should show: ./docker/clickhouse/...
```

---

### 5. Missing Compose Directory Scripts

**Problem:** The `compose/` directory was empty, causing web container to fail with:
```
/usr/local/bin/docker-entrypoint.sh: 101: exec: /compose/start: not found
```

**Solution:** Copy required scripts from `bin/` to `compose/`.

```bash
sudo mkdir -p ~/posthog/compose
sudo cp ~/posthog/bin/start ~/posthog/compose/
sudo cp ~/posthog/bin/start-backend ~/posthog/compose/
sudo cp ~/posthog/bin/start-worker ~/posthog/compose/
sudo cp ~/posthog/bin/start-frontend ~/posthog/compose/
sudo chmod +x ~/posthog/compose/*
sudo chown -R ubuntu:ubuntu ~/posthog/compose
```

---

### 6. Caddy Not Using Domain for SSL

**Problem:** Caddy was configured to listen on `localhost:8000` instead of the domain, preventing SSL certificate provisioning.

**Original Caddyfile (generated dynamically):**
```
http://localhost:8000 {
    ...
}
```

**Solution:** Create custom Caddyfile and override proxy service.

**Step 1:** Create Caddyfile with domain:

```bash
mkdir -p ~/posthog/docker/proxy
cat > ~/posthog/docker/proxy/Caddyfile << 'EOF'
posthog.drivex.dev {
    @replay-capture {
        path /s
        path /s/
        path /s/*
    }

    @capture {
        path /e
        path /e/
        path /e/*
        path /i/v0
        path /i/v0/
        path /i/v0/*
        path /batch
        path /batch/
        path /batch/*
        path /capture
        path /capture/
        path /capture/*
    }

    @flags {
        path /flags
        path /flags/
        path /flags/*
    }

    @webhooks {
        path /public/webhooks
        path /public/webhooks/
        path /public/webhooks/*
        path /public/m/
        path /public/m/*
    }

    handle @capture {
        reverse_proxy capture:3000
    }

    handle @replay-capture {
        reverse_proxy replay-capture:3000
    }

    handle @flags {
        reverse_proxy feature-flags:3001
    }

    handle @webhooks {
        reverse_proxy plugins:6738
    }

    handle {
        reverse_proxy web:8000
    }
}
EOF
```

**Step 2:** Modify proxy service in `docker-compose.hobby.yml`:

Replace the proxy section (around line 156) with:

```yaml
    proxy:
        image: caddy:2
        restart: unless-stopped
        ports:
            - '80:80'
            - '443:443'
        volumes:
            - caddy-data:/data
            - caddy-config:/config
            - ./docker/proxy/Caddyfile:/etc/caddy/Caddyfile:ro
        command: caddy run --config /etc/caddy/Caddyfile
        depends_on:
            - web
```

**Verification:** Check SSL certificate provisioning in logs:
```bash
sudo docker compose -f docker-compose.hobby.yml logs proxy --tail=30
```

Should show:
```
"certificate obtained successfully", "identifier": "posthog.drivex.dev"
```

---

### 7. GeoIP Download Blocking Web Startup

**Problem:** Web container stuck in infinite retry loop trying to download GeoIP database.

**Error:**
```
./bin/download-mmdb: line 22: brotli: command not found
curl: (23) Failure writing output to destination
Download failed after retries
```

**Solution:** Modify the start script to skip GeoIP download and use gunicorn.

```bash
cat > ~/posthog/compose/start << 'EOF'
#!/bin/bash
set -e

export DISABLE_MMDB=1
export SKIP_MMDB_DOWNLOAD=1

echo "ğŸš€ Starting PostHog..."

# Run migrations
python manage.py migrate --noinput 2>/dev/null || true

# Start with gunicorn
exec gunicorn posthog.wsgi:application --bind 0.0.0.0:8000 --workers 2 --timeout 120
EOF

chmod +x ~/posthog/compose/start
```

---

## Final Directory Structure

```
~/posthog/
â”œâ”€â”€ .env                              # Environment configuration
â”œâ”€â”€ docker-compose.hobby.yml          # Modified compose file
â”œâ”€â”€ compose/
â”‚   â”œâ”€â”€ start                         # Modified start script
â”‚   â”œâ”€â”€ start-backend
â”‚   â”œâ”€â”€ start-worker
â”‚   â””â”€â”€ start-frontend
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ clickhouse/
â”‚   â”‚   â”œâ”€â”€ config.xml               # ClickHouse config
â”‚   â”‚   â”œâ”€â”€ users.xml
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ proxy/
â”‚       â””â”€â”€ Caddyfile                # Custom Caddy config with domain
â””â”€â”€ ...
```

---

## Final `.env` File

```env
POSTHOG_SECRET=<generated-secret>
ENCRYPTION_SALT_KEYS=<generated-salt>
DOMAIN=posthog.drivex.dev
EXTERNAL_URL=https://posthog.drivex.dev
POSTHOG_APP_TAG=latest
REGISTRY_URL=posthog/posthog
TLS_BLOCK=
```

---

## Useful Commands

### Start PostHog
```bash
cd ~/posthog
sudo docker compose -f docker-compose.hobby.yml up -d
```

### Stop PostHog
```bash
cd ~/posthog
sudo docker compose -f docker-compose.hobby.yml down
```

### View Logs
```bash
# All services
sudo docker compose -f docker-compose.hobby.yml logs -f

# Specific service
sudo docker compose -f docker-compose.hobby.yml logs web --tail=50
sudo docker compose -f docker-compose.hobby.yml logs proxy --tail=50
```

### Check Status
```bash
sudo docker compose -f docker-compose.hobby.yml ps
```

### Restart a Service
```bash
sudo docker compose -f docker-compose.hobby.yml restart web
sudo docker compose -f docker-compose.hobby.yml restart proxy
```

### Full Reset (if needed)
```bash
cd ~/posthog
sudo docker compose -f docker-compose.hobby.yml down -v --remove-orphans
sudo docker system prune -af --volumes
sudo systemctl restart docker
sleep 10
sudo docker compose -f docker-compose.hobby.yml up -d
```

---

## AWS Security Group Requirements

Ensure your EC2 security group allows:

| Port | Protocol | Source    | Purpose                        |
|------|----------|-----------|--------------------------------|
| 80   | TCP      | 0.0.0.0/0 | HTTP (Let's Encrypt challenge) |
| 443  | TCP      | 0.0.0.0/0 | HTTPS (PostHog access)         |
| 22   | TCP      | Your IP   | SSH access                     |

---

## Route53 DNS Configuration

- **Record Type:** A
- **Name:** posthog.drivex.dev
- **Value:** EC2 Public IP Address
- **TTL:** 300

---

## Verification Checklist

- [ ] `https://posthog.drivex.dev` loads PostHog UI
- [ ] SSL certificate is valid (padlock icon in browser)
- [ ] All core containers running: `web`, `proxy`, `clickhouse`, `kafka`, `redis`, `db`
- [ ] Can create admin account and log in

---

## Known Issues (Non-Critical)

Some services may show "Restarting" status initially:
- `feature-flags` - Stabilizes after web is fully up
- `property-defs-rs` - Stabilizes after ClickHouse is ready
- `temporal-django-worker` - Stabilizes after migrations complete

These typically resolve within 5-10 minutes of initial startup.

---

## Contact

For PostHog documentation: https://posthog.com/docs/self-host

---

*Document created: November 24, 2025* 

# PostHog Troubleshooting - ClickHouse Tables Missing

**Issue Date:** November 25, 2025  
**Instance:** `posthog.drivex.dev`  
**Server:** AWS EC2 (Ubuntu 24.04)

---

## Problem Description

After successful deployment, PostHog UI was accessible but non-functional:
- Dashboard showed "A server error occurred" on all insights
- All analytics widgets displayed server errors
- Plugin configuration page showed: "Load hog function plugins destinations failed: A server error occurred"

### Symptoms in Screenshots
- Daily Active Users (DAUs) widget: "A server error occurred"
- Weekly Active Users (WAUs) widget: "A server error occurred"
- Growth Accounting widget: "A server error occurred"
- Retention widget: "A server error occurred"
- Error banner: "Load hog function plugins destinations failed: A server error occurred"

---

## Root Cause Analysis

### Investigation Steps

**1. Check Container Status:**
```bash
cd ~/posthog
sudo docker compose -f docker-compose.hobby.yml ps
```

**Findings:**
- Most containers running normally
- `feature-flags` container: **Restarting (101)** - continuously failing
- `temporal-django-worker` container: **Restarting (127)** - continuously failing
- `worker` container: Just restarted

**2. Check Web Service Logs:**
```bash
sudo docker compose -f docker-compose.hobby.yml logs web --tail=100
```

**Critical Error Found:**
```
clickhouse_driver.errors.ServerException: Code: 60.
DB::Exception: Table posthog.app_metrics does not exist.
```

**3. Root Cause Identified:**

The ClickHouse database was running, but **PostHog's schema (tables) were never initialized**. The application was trying to query tables like:
- `posthog.app_metrics`
- `posthog.events`
- `posthog.persons`
- `posthog.sessions`

But these tables didn't exist in the ClickHouse database.

### Why This Happened

When PostHog is deployed, it requires two migration steps:
1. **PostgreSQL migrations** - for user accounts, projects, teams (completed automatically)
2. **ClickHouse migrations** - for analytics data tables (was NOT run)

The ClickHouse migrations must be explicitly triggered using:
```bash
python manage.py migrate_clickhouse
```

---

## Solution

### Step 1: Run ClickHouse Migrations

```bash
cd ~/posthog

# Execute ClickHouse migrations inside the web container
sudo docker compose -f docker-compose.hobby.yml exec web python manage.py migrate_clickhouse
```

**Expected Output:**
```
{'event': "\n                You indicated your instance is behind a proxy (IS_BEHIND_PROXY env var),\n                but you haven't configured any trusted proxies. See\n                https://posthog.com/docs/configuring-posthog/running-behind-proxy for details.\n            ", 'timestamp': '2025-11-25T06:01:14.868910Z', 'logger': 'posthog.settings.access', 'level': 'warning', 'pid': 584, 'tid': 139728410925952}
2025-11-25T06:01:15.870543Z [warning  ] initialized clickhouse users: DEFAULT,API,APP [root] pid=584 tid=139728410925952
...
âœ… Migration successful
```

**Note:** Warnings about proxy configuration and GeoIP database are non-critical and can be ignored.

---

### Step 2: Verify Tables Were Created

```bash
# List all tables in the posthog database
sudo docker compose -f docker-compose.hobby.yml exec clickhouse clickhouse-client --query "SHOW TABLES FROM posthog"
```

**Expected Output (partial list):**
```
app_metrics
events
person
person_distinct_id
session_recording_events
sessions
groups
cohortpeople
plugin_log_entries
...
```

---

### Step 3: Restart Failing Services

After tables are created, restart services that were stuck in restart loops:

```bash
cd ~/posthog

# Restart the affected services
sudo docker compose -f docker-compose.hobby.yml restart feature-flags
sudo docker compose -f docker-compose.hobby.yml restart temporal-django-worker
sudo docker compose -f docker-compose.hobby.yml restart worker
sudo docker compose -f docker-compose.hobby.yml restart web

# Wait for services to stabilize
sleep 30
```

---

### Step 4: Verify All Services Are Running

```bash
sudo docker compose -f docker-compose.hobby.yml ps
```

**Expected Output:**
All services should show **"Up"** status (except brief restarts):
```
NAME                               STATUS
posthog-web-1                      Up
posthog-clickhouse-1               Up
posthog-db-1                       Up (healthy)
posthog-redis-1                    Up (healthy)
posthog-kafka-1                    Up (healthy)
posthog-plugins-1                  Up
posthog-worker-1                   Up
posthog-feature-flags-1            Up
posthog-temporal-django-worker-1   Up
...
```

---

### Step 5: Test in Browser

1. Navigate to `https://posthog.drivex.dev`
2. Refresh the dashboard
3. Verify insights load without errors:
   - Daily Active Users (DAUs) should show graph or "No data" (not error)
   - Weekly Active Users (WAUs) should show graph or "No data" (not error)
   - Growth Accounting should display properly
   - Retention widget should display properly
4. Check plugin configuration page - should load without error banner

---

## Alternative Solution (If Migration Command Fails)

If the `migrate_clickhouse` command doesn't exist or fails, use the full reset approach:

```bash
cd ~/posthog

# Stop all services
sudo docker compose -f docker-compose.hobby.yml down

# Remove ClickHouse data (forces reinitialization)
sudo rm -rf ~/posthog/data/clickhouse

# Start ClickHouse first
sudo docker compose -f docker-compose.hobby.yml up -d clickhouse
sleep 30

# Start web service (will trigger schema initialization)
sudo docker compose -f docker-compose.hobby.yml up -d web
sleep 60

# Check if tables were created
sudo docker compose -f docker-compose.hobby.yml exec clickhouse clickhouse-client --query "SHOW TABLES FROM posthog"

# Start all remaining services
sudo docker compose -f docker-compose.hobby.yml up -d
```

---

## Verification Checklist

After applying the fix, verify:

- [ ] `migrate_clickhouse` command completed successfully
- [ ] ClickHouse tables exist (40+ tables should be present)
- [ ] All containers show "Up" status
- [ ] No containers stuck in restart loops
- [ ] Dashboard insights load without "server error" messages
- [ ] Can create new insights
- [ ] Plugin configuration page loads without errors
- [ ] No ClickHouse table errors in web service logs

---

## Monitoring for Errors

To monitor for any remaining issues:

```bash
# Watch web service logs in real-time
sudo docker compose -f docker-compose.hobby.yml logs web -f --tail=50

# Watch ClickHouse logs
sudo docker compose -f docker-compose.hobby.yml logs clickhouse -f --tail=50

# Watch all logs
sudo docker compose -f docker-compose.hobby.yml logs -f
```

**What to Look For:**
- âŒ **Bad:** `Table posthog.XXX does not exist`
- âŒ **Bad:** `Code: 60. DB::Exception`
- âœ… **Good:** Normal query logs without table errors
- âœ… **Good:** Successful API requests in web logs

---

## Understanding the Error

### What Happened Behind the Scenes

1. **PostgreSQL (db container):** Stores user accounts, projects, teams, configurations
   - Migrations ran automatically âœ…
   
2. **ClickHouse (clickhouse container):** Stores analytics events, persons, sessions
   - Container started âœ…
   - Database created âœ…
   - **Tables NOT created** âŒ
   
3. **Web Service:** Tried to query ClickHouse tables
   - PostgreSQL queries worked fine âœ…
   - ClickHouse queries failed (tables missing) âŒ

### Why Manual Migration Was Needed

PostHog's docker-compose setup runs PostgreSQL migrations automatically via the `start` script, but ClickHouse migrations must be triggered separately because:
- ClickHouse schema is more complex
- Analytics tables require specific partitioning
- Migration timing depends on ClickHouse being fully ready

---

## Prevention for Future Deployments

### Fresh Installation Checklist

When deploying PostHog from scratch:

1. Start all services:
   ```bash
   sudo docker compose -f docker-compose.hobby.yml up -d
   ```

2. Wait for all services to be ready (2-3 minutes):
   ```bash
   sleep 180
   ```

3. **Always run ClickHouse migrations:**
   ```bash
   sudo docker compose -f docker-compose.hobby.yml exec web python manage.py migrate_clickhouse
   ```

4. Verify tables exist:
   ```bash
   sudo docker compose -f docker-compose.hobby.yml exec clickhouse clickhouse-client --query "SHOW TABLES FROM posthog" | wc -l
   ```
   Should return 40+ tables

5. Restart services if needed:
   ```bash
   sudo docker compose -f docker-compose.hobby.yml restart feature-flags temporal-django-worker worker
   ```

---

## Related Issues

### PostgreSQL Errors (Non-Critical)

The PostgreSQL logs showed duplicate constraint errors:
```
ERROR: ON CONFLICT DO UPDATE command cannot affect row a second time
```

**Analysis:** This is a known issue with concurrent property definition updates. It's non-critical and doesn't affect functionality. The errors appear when multiple workers try to update the same property definitions simultaneously.

**Impact:** None - these are handled gracefully by PostHog's code.

---

## Additional Commands Reference

### Check ClickHouse Database Size
```bash
sudo docker compose -f docker-compose.hobby.yml exec clickhouse clickhouse-client --query "
SELECT 
    database,
    formatReadableSize(sum(bytes)) AS size
FROM system.parts
WHERE database = 'posthog'
GROUP BY database"
```

### Check Table Row Counts
```bash
sudo docker compose -f docker-compose.hobby.yml exec clickhouse clickhouse-client --query "
SELECT 
    table,
    formatReadableQuantity(total_rows) AS rows
FROM system.tables
WHERE database = 'posthog'
ORDER BY total_rows DESC
LIMIT 10"
```

### Test ClickHouse Connectivity
```bash
sudo docker compose -f docker-compose.hobby.yml exec clickhouse clickhouse-client --query "SELECT version()"
```

### Manual Table Creation (Last Resort)

If migrations fail completely, you can manually create tables by running the SQL scripts:
```bash
# Access ClickHouse client
sudo docker compose -f docker-compose.hobby.yml exec clickhouse clickhouse-client

# Inside the client, check for schema files
# Note: This is a last resort and should generally not be needed
```

---

## Summary

**Problem:** PostHog UI accessible but all analytics features showing "A server error occurred"

**Root Cause:** ClickHouse database tables were not initialized during deployment

**Solution:** Run `python manage.py migrate_clickhouse` inside the web container

**Result:** All tables created, services stabilized, analytics working

**Time to Fix:** ~2-3 minutes

**Prevention:** Always run ClickHouse migrations after initial deployment

---

## References

- PostHog Self-Hosted Docs: https://posthog.com/docs/self-host
- ClickHouse Documentation: https://clickhouse.com/docs
- PostHog GitHub Issues: https://github.com/PostHog/posthog/issues

---

*Document created: November 25, 2025*  
*Issue resolved: November 25, 2025* 

///////////////////////////////////////// commands to check Shai-Hulud Attack Detection Script on posthog ///////////////////////////////////////////////////

echo "=========================================="
echo "Shai-Hulud Attack Detection Script"
echo "=========================================="

echo ""
echo "1. Checking for malicious files..."
MALICIOUS=$(find /home -name "setup_bun.js" -o -name "bun_environment.js" -o -name "cloud.json" -o -name "contents.json" -o -name "environment.json" -o -name "truffleSecrets.json" 2>/dev/null | grep -v ".git")
if [ -z "$MALICIOUS" ]; then
    echo "   âœ… No malicious files found"
else
    echo "   âŒ WARNING: Malicious files detected:"
    echo "$MALICIOUS"
fi

echo ""
echo "2. Checking for node_modules..."
NODEMOD=$(find /home/ubuntu -type d -name "node_modules" 2>/dev/null)
if [ -z "$NODEMOD" ]; then
    echo "   âœ… No node_modules directories (no npm packages installed)"
else
    echo "   âš ï¸  node_modules found at:"
    echo "$NODEMOD"
fi

echo ""
echo "3. Checking npm logs for suspicious activity..."
if [ -d ~/.npm/_logs ]; then
    SUSPICIOUS=$(grep -r "shai\|hulud\|preinstall" ~/.npm/_logs 2>/dev/null)
    if [ -z "$SUSPICIOUS" ]; then
        echo "   âœ… No suspicious entries in npm logs"
    else
        echo "   âŒ WARNING: Suspicious npm log entries:"
        echo "$SUSPICIOUS"
    fi
else
    echo "   âœ… No npm logs directory (npm not used)"
fi

echo ""
echo "4. Checking GitHub Actions workflows..."
WORKFLOWS=$(find /home -name "discussion.yaml" -path "*/.github/workflows/*" 2>/dev/null)
if [ -z "$WORKFLOWS" ]; then
    echo "   âœ… No suspicious GitHub workflow files"
else
    echo "   âš ï¸  Found discussion.yaml workflow (could be malicious):"
    echo "$WORKFLOWS"
fi

echo ""
echo "=========================================="
echo "Scan complete!"
echo "=========================================="


//////////////////////////////////////////////////////////////////////////////////////////////////////////////
